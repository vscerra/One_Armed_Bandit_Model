# One_Armed_Bandit_Model

Reinforcement learning (RL) is the process by which reward is associated with an action - that is, learning the best behavioral choice to maximize reward. We tend to base our decisions on what we think is the most likely choice to earn a reward, and given that most rewards are probabalistic (i.e., there is some uncertainty in whether or not the action will lead to reward), we rely on feedback to guide our future choices. RL has been used to explain activity in the brain's dopaminergic system. 

A simple RL model is the one-armed-bandit model, which essentially simluates a slot machine (the bandit). The machine pays out with a reward rate, $p_{win}$, and for simplicity, we model the rewards as 1 or 0. Given this set-up, the value, **V**, of reward for for a given number of plays, $T$, is simply the mean success over $T$ plays, each with a reward probabiity of $p_{win}$. The above scenario is called the "Simple Model" of reward. 

The drawback of the Simple Model is that it requires an average over all previous "plays" and it is unrealistic to compute that average over thousands of plays. Instead, a slight tweak of the calculation can give us the exact same value mathematically, but with far fewer numbers to calculate. This **Error Prediction Model** gives the same value for reward at on trial $t$ by using the value of reward at $t-1$ as a substitute for all of the previous calculation, and weights that value by the number of trials: $V_{t}$ = $V_{t-1}$ + (1/t) ($r_{t}$ - $V_{t-1}$). 

One further complication of the above reward predcition models is that when $p_{win}$ changes, the models are slow to adapt to that change, as they take all of the reward history into account. That is to say, if the slot machine you were playing suddenly went from paying out 85% of the time to 15% of the time (sometimes called a "bait and switch" schema), the Simple and Prediction Error models would be delayed in adapting play. One way to combat this, and adjust more quickly to changes in reward probability is to implement a fixed learning rate into your model. This replaces the (1/t) weighting term in the above Error Prediction equation with a learning rate term ($\alpha$) and substitutes a dependence on all previous trials with a constant, thus adapting more quickly to new reward probabilities. 

The script in `One_Armed_Bandit_Model` implements both the Simple Model and the Error Prediction Model to demonstrate that both models produce the same reward values over multiple trials. Additionally, it incorporates a Fixed Learning Rate model to demonstrate its efficacy in adapting to bait-and-switch paradigms. 

